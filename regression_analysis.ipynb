{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Crown Closure in TFL 38 Using VRI Data\n",
    "\n",
    "#### 1. Problem Definition:\n",
    "Crown closure is a key forest metric representing the proportion of ground covered by tree canopies. It influences habitat suitability, light availability, and stand productivity. This project aims to develop a regression-based machine learning model to predict crown closure using key forest inventory attributes from Vegetation Resources Inventory (VRI) data. This project follows a structured machine learning workflow, including data preprocessing, exploratory analysis, feature selection, model training, and evaluation, to opimize model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Description and Preprocessing\n",
    "The dataset used in this project is sourced from the British Columbia Vegetation Resources Inventory (VRI), which provides stand-level forest attributes collected through remote sensing and field surveys. The target variable for this analysis is crown closure (%), which represents the proportion of the forest floor covered by tree canopies. The predictor variables selected for the model are basal area (m²/ha), VRI live stems per hectare, projected age (years), projected height (m), and whole stem biomass per hectare (Mg/ha). \n",
    "\n",
    "The original dataset contains VRI data for the whole province of British Columbia. I decided to filter my data in ArcGIS Pro to exclude all projects except for TFL 38, a tree farm licence located northwest of Squamish, BC. I also filtered out all unnecessary fields. I then exported my filtered data to a new feature class named \"tfl_38_VRI_cleaned\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Load Required Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Load and Inspect Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile (update user specific file path)\n",
    "shapefile_path = r\"C:\\Users\\ryanj\\Desktop\\project_1\\tfl_38_VRI_cleaned\\tfl_38_VRI_cleaned.shp\"\n",
    "tfl_38_VRI = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(tfl_38_VRI.head())\n",
    "\n",
    "# Check the column names and data types\n",
    "print(tfl_38_VRI.info())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = tfl_38_VRI.isnull().sum()\n",
    "\n",
    "# Print missing values or a message if none\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values\")\n",
    "else:\n",
    "    print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values, but I noticed that some of the column names are incorrect. I also want to drop some of the unnecessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying column names\n",
    "print(tfl_38_VRI.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Drop Unnecessary Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep\n",
    "cols_to_keep = ['FEATURE_ID', 'CROWN_CLOS', 'BASAL_AREA', 'VRI_LIVE_S', \n",
    "                'PROJ_AGE_1', 'PROJ_HEIGH', 'WHOLE_STEM', 'Shape_Leng', \n",
    "                'Shape_Area', 'geometry']\n",
    "\n",
    "# Create a new DataFrame with only the specified columns\n",
    "VRI_cleaned = tfl_38_VRI[cols_to_keep]\n",
    "\n",
    "# Check the first few rows to confirm\n",
    "print(VRI_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns are not named correctly:\n",
    "- `CROWN_CLOS` should be: `CROWN_CLOSURE`\n",
    "- `VRI_LIVE_S` should be: `VRI_LIVE_STEMS`\n",
    "- `PROJ_HEIG` should be: `PROJ_HEIGHT`\n",
    "- `WHOLE_STEM` should be: `WHOLE_STEMS`\n",
    "- `Shape_Leng` should be: `Shape_Length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix variable names\n",
    "VRI_cleaned = VRI_cleaned.rename(columns={\n",
    "    'CROWN_CLOS': 'CROWN_CLOSURE',\n",
    "    'VRI_LIVE_S': 'VRI_LIVE_STEMS',\n",
    "    'PROJ_HEIGH': 'PROJ_HEIGHT_1',  \n",
    "    'WHOLE_STEM': 'WHOLE_STEM_BIOMASS',\n",
    "    'Shape_Leng': 'Shape_Length'\n",
    "})\n",
    "\n",
    "# Print the updated variable names to verify changes\n",
    "print(VRI_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Basic Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\n--- Basic Statistics ---\")\n",
    "print(VRI_cleaned.describe())\n",
    "\n",
    "# Check the Coordinate Reference System (CRS)\n",
    "print(\"\\n--- CRS ---\")\n",
    "print(VRI_cleaned.crs)\n",
    "\n",
    "print(VRI_cleaned.columns)\n",
    "\n",
    "# Verify spatial data integrity:\n",
    "VRI_cleaned.plot(figsize=(10, 10), edgecolor=\"black\")\n",
    "plt.title(\"TFL 38 VRI - Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that my data is cleaned and verified, I will move on to exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data type\n",
    "column_data_types = VRI_cleaned.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Explore Variable Properties and Relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics\n",
    "summary_stats = VRI_cleaned.describe()\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Create scatter plot matrix\n",
    "sns.pairplot(VRI_cleaned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_columns = VRI_cleaned.select_dtypes(include=['float64', 'int64', 'int32'])\n",
    "\n",
    "# Compute the correlation matrix for numeric columns\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "# Display the full correlation matrix\n",
    "print(\"\\n--- Correlation Matrix ---\")\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Correlation coefficient results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong Correlations:\n",
    "- **Basal Area & Whole Stem Biomass (0.9065)** – Stands with a higher basal area tend to have greater whole stem biomass.\n",
    "- **Projected Height & Whole Stem Biomass (0.9166)** – Taller trees generally contribute to greater biomass.\n",
    "- **Basal Area & Projected Height (0.7981)** – More basal area is associated with taller trees.\n",
    "- **Shape Length & Shape Area (0.8867)** –  Greater length correlates with its area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moderate Correlations:\n",
    "- **Crown Closure & Basal Area (0.7226)** – Denser crown cover is linked to higher basal area.\n",
    "- **Crown Closure & Whole Stem Biomass (0.6378)** – More crown closure generally leads to more biomass.\n",
    "- **Projected Age & Projected Height (0.4284)** – Older stands tend to be taller, but the correlation is not strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms with KDE for Crown Closure vs Basal Area, and Crown Closure vs Whole Stem Biomass\n",
    "crown_closure = VRI_cleaned['CROWN_CLOSURE']\n",
    "basal_area = VRI_cleaned['BASAL_AREA'] \n",
    "whole_stem_biomass = VRI_cleaned['WHOLE_STEM_BIOMASS']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Crown Closure vs Basal Area\n",
    "sns.histplot(crown_closure, kde=True, color='salmon', bins=30, ax=axes[0], label='Crown Closure')\n",
    "sns.histplot(basal_area, kde=True, color='bisque', bins=30, ax=axes[0], label='Basal Area')\n",
    "axes[0].set_title(\"Crown Closure vs Basal Area\")\n",
    "axes[0].set_xlabel(\"Values\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Crown Closure vs Whole Stem Biomass\n",
    "sns.histplot(crown_closure, kde=True, color='orange', bins=30, ax=axes[1], label='Crown Closure')\n",
    "sns.histplot(whole_stem_biomass, kde=True, color='chocolate', bins=30, ax=axes[1], label='Whole Stem Biomass')\n",
    "axes[1].set_title(\"Crown Closure vs Whole Stem Biomass\")\n",
    "axes[1].set_xlabel(\"Values\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution:\n",
    "- **Crown Closure vs Basal Area** - Normal Distribution\n",
    "- **Crown Closure vs Whole Stem Biomass** - Right-Skewed Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Data Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness of the variables\n",
    "skewed_features = VRI_cleaned.skew()\n",
    "print(skewed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code returned features from the \"geometry\" variable. The output confirms that the 'geometry' variable contains valid polygon features. Since transformations are typically applied to numerical attributes to correct skewness, and the geometry itself does not require such adjustments, variable transformation is not necessary in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Dimensionality Reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) helps reduce the number of features while retaining as much variance as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical features (excluding 'CROWN_CLOSURE' and 'geometry')\n",
    "num_features = VRI_cleaned.drop(columns=['CROWN_CLOSURE', 'geometry'])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "num_features_scaled = scaler.fit_transform(num_features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(num_features_scaled)\n",
    "\n",
    "# Explained variance plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Determine how many components explain 95% of the variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1  # Get the first index where variance ≥ 95%\n",
    "\n",
    "print(f\"Optimal number of components to retain 95% variance: {n_components}\")\n",
    "\n",
    "# Apply PCA with the selected number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(num_features_scaled)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_pca = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "df_pca['CROWN_CLOSURE'] = VRI_cleaned['CROWN_CLOSURE']  # Retain target variable\n",
    "\n",
    "print(f\"New dataset shape after PCA: {df_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Data Augmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now enhance the dataset by artificially increasing the number of samples to improve model performance. To achieve this, I will apply Gaussian Noise Injection, subtly perturbing numerical features to generate synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of synthetic samples (20% of original dataset)\n",
    "num_synthetic_samples = int(0.2 * len(df_pca))\n",
    "\n",
    "# Select numerical features excluding target variable\n",
    "X_numeric = df_pca.drop(columns=['CROWN_CLOSURE'])\n",
    "\n",
    "# Generate synthetic data by adding Gaussian noise\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(loc=0, scale=0.01, size=(num_synthetic_samples, X_numeric.shape[1]))  # Small noise\n",
    "synthetic_samples = X_numeric.sample(n=num_synthetic_samples, replace=True).values + noise\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_synthetic = pd.DataFrame(synthetic_samples, columns=X_numeric.columns)\n",
    "\n",
    "# Assign synthetic target values from sampled real values\n",
    "df_synthetic['CROWN_CLOSURE'] = df_pca['CROWN_CLOSURE'].sample(n=num_synthetic_samples, replace=True).values\n",
    "\n",
    "# Append to original dataset\n",
    "df_augmented = pd.concat([df_pca, df_synthetic], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset shape after augmentation: {df_augmented.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Describe the Regression Algorithms to be Used:\n",
    "\n",
    "a) **Linear Regression**\n",
    "-   **Strengths:** Simple, interpretable, fast to train.\n",
    "-   **Weaknesses:** Assumes linearity, sensitive to outliers.\n",
    "-   **Assumptions:** Linearity, homoscedasticity, normality of errors.\n",
    "-   **Limitations:** Poor performance with complex, non-linear relationships.\n",
    "\n",
    "b) **Random Forest Regression**\n",
    "-   **Strengths:** Handles non-linear data, robust to overfitting.\n",
    "-   **Weaknesses:** Computationally expensive, less interpretable.\n",
    "-   **Assumptions:** Uses ensemble decision trees to improve predictions.\n",
    "-   **Limitations:** Performance can degrade with irrelevant features.\n",
    "\n",
    "c) **Support Vector Regression (SVR)**\n",
    "-   **Strengths:** Good for high-dimensional spaces, handles non-linearity.\n",
    "-   **Weaknesses:** Sensitive to kernel choice, computationally expensive.\n",
    "-   **Assumptions:** Relies on kernel functions for non-linear modeling.\n",
    "-   **Limitations:** Struggles with noisy data and large datasets.\n",
    "\n",
    "d) **K-Nearest Neighbors (KNN) Regression**\n",
    "-   **Strengths:** Simple, non-parametric, captures local patterns.\n",
    "-   **Weaknesses:** Slow predictions, sensitive to irrelevant features.\n",
    "-   **Assumptions:** Similar instances have similar target values.\n",
    "-   **Limitations:** Struggles with high-dimensional data, computationally expensive.\n",
    "\n",
    "e) **Gradient Boosting Regression**\n",
    "-   **Strengths:** Handles non-linear relationships, high predictive accuracy.\n",
    "-   **Weaknesses:** Computationally expensive, requires careful tuning.\n",
    "-   **Assumptions:** Uses weak learners to iteratively improve the model.\n",
    "-   **Limitations:** Can overfit without proper tuning, less interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Select Most Suitable Regression Algorithms\n",
    "\n",
    "Based on my project and the above model descriptions, the three most suitable algorithms are:\n",
    "\n",
    "a) **Random Forest Regression**\n",
    "\n",
    "**Reason:** This model is well-suited for handling complex, non-linear relationships in data and is robust to overfitting. Additionally, it can handle a variety of features, including numerical and categorical, which is important in geospatial data analysis.\n",
    "\n",
    "b) **Support Vector Regression (SVR)**\n",
    "\n",
    "**Reason:** SVR is effective for high-dimensional data and can capture non-linear relationships. It's also useful when there's a need to generalize well to unseen data, which can be crucial for model accuracy.\n",
    "\n",
    "c) **Gradient Boosting Regression**\n",
    "\n",
    "**Reason:** This algorithm is known for its high predictive accuracy, especially with complex datasets. Gradient Boosting can handle non-linearities well and provides a good balance between bias and variance when tuned properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Create Training/Testing Sets (Using an 80/20 Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target variable (y)\n",
    "X = df_pca.drop(columns='CROWN_CLOSURE')  # Features after PCA\n",
    "y = df_pca['CROWN_CLOSURE']  # Target variable\n",
    "\n",
    "# Split into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Train the Regression Models (with Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "svr_model = SVR()\n",
    "gbr_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Train the models with cross-validation (using 5 folds)\n",
    "rf_cv = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "svr_cv = cross_val_score(svr_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "gbr_cv = cross_val_score(gbr_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Output the mean cross-validation scores\n",
    "print(f\"Random Forest CV MSE: {np.mean(rf_cv)}\")\n",
    "print(f\"SVR CV MSE: {np.mean(svr_cv)}\")\n",
    "print(f\"Gradient Boosting CV MSE: {np.mean(gbr_cv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Results:\n",
    "\n",
    "-   **MSE Significance**: A lower MSE indicates better model performance, as it means the model's predictions are closer to the true values. A model with a less negative MSE is preferred.\n",
    "\n",
    "-   **Comparison**: Random Forest has the lowest MSE, suggesting it performs better than the other two models in terms of minimizing prediction errors. SVR has the highest MSE, indicating it is performing the worst among the three models. Gradient Boosting performed better than SVR but worse than Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Tune the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for each model\n",
    "\n",
    "# Random Forest parameter distribution\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# SVR parameter distribution\n",
    "svr_param_dist = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameter distribution\n",
    "gbr_param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 4],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for each model\n",
    "\n",
    "# Random Forest Randomized Search\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=10,  # Limits the number of random samples\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,  # Uses all available CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# SVR Randomized Search\n",
    "svr_random_search = RandomizedSearchCV(\n",
    "    estimator=svr_model,\n",
    "    param_distributions=svr_param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Gradient Boosting Randomized Search\n",
    "gbr_random_search = RandomizedSearchCV(\n",
    "    estimator=gbr_model,\n",
    "    param_distributions=gbr_param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit models\n",
    "print(\"Tuning Random Forest...\")\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "print(\"Best Random Forest Params:\", rf_random_search.best_params_)\n",
    "\n",
    "print(\"Tuning SVR...\")\n",
    "svr_random_search.fit(X_train, y_train)\n",
    "print(\"Best SVR Params:\", svr_random_search.best_params_)\n",
    "\n",
    "print(\"Tuning Gradient Boosting...\")\n",
    "gbr_random_search.fit(X_train, y_train)\n",
    "print(\"Best Gradient Boosting Params:\", gbr_random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with best parameters\n",
    "rf_best = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=None, \n",
    "    max_features='sqrt', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svr_best = SVR(\n",
    "    kernel='rbf',\n",
    "    gamma='auto',\n",
    "    C=100\n",
    ")\n",
    "\n",
    "gbr_best = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the models\n",
    "rf_best.fit(X_train, y_train)\n",
    "svr_best.fit(X_train, y_train)\n",
    "gbr_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "rf_preds = rf_best.predict(X_test)\n",
    "svr_preds = svr_best.predict(X_test)\n",
    "gbr_preds = gbr_best.predict(X_test)\n",
    "\n",
    "# Evaluate performance using Mean Squared Error\n",
    "rf_mse = mean_squared_error(y_test, rf_preds)\n",
    "svr_mse = mean_squared_error(y_test, svr_preds)\n",
    "gbr_mse = mean_squared_error(y_test, gbr_preds)\n",
    "\n",
    "print(f\"Final Random Forest MSE: {rf_mse}\")\n",
    "print(f\"Final SVR MSE: {svr_mse}\")\n",
    "print(f\"Final Gradient Boosting MSE: {gbr_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Model Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Compute Root Mean Squared Error (RMSE) Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE values from MSE\n",
    "rf_rmse = np.sqrt(67.81497609147608)\n",
    "svr_rmse = np.sqrt(85.1698000519334)\n",
    "gbr_rmse = np.sqrt(76.12485597094468)\n",
    "\n",
    "# Store RMSE values\n",
    "rmse_values = {\n",
    "    \"Random Forest\": rf_rmse,\n",
    "    \"SVR\": svr_rmse,\n",
    "    \"Gradient Boosting\": gbr_rmse\n",
    "}\n",
    "\n",
    "# Print RMSE values\n",
    "for model, rmse in rmse_values.items():\n",
    "    print(f\"{model} RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for RMSE comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(rmse_values.keys(), rmse_values.values(), color=['blue', 'red', 'green'])\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Model Comparison Based on RMSE\")\n",
    "plt.ylim(min(rmse_values.values()) - 2, max(rmse_values.values()) + 2)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the RMSE values on top of each bar\n",
    "for i, value in enumerate(rmse_values.values()):\n",
    "    plt.text(i, value + 0.5, f\"{value:.2f}\", ha='center', fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of y_test (target variable)\n",
    "std_y_test = np.std(y_test)\n",
    "\n",
    "# Print the standard deviation of y_test\n",
    "print(\"\\n--- Target Variable Standard Deviation ---\")\n",
    "print(std_y_test)\n",
    "\n",
    "# Compare RMSE to Standard Deviation\n",
    "print(\"\\n--- Comparison between RMSE and Standard Deviation ---\")\n",
    "for model_name, rmse in rmse_values.items():\n",
    "    if rmse < std_y_test:\n",
    "        print(f\"The RMSE for {model_name} ({rmse:.2f}) is lower than the standard deviation of the target variable ({std_y_test:.2f}), indicating good model performance.\")\n",
    "    elif rmse == std_y_test:\n",
    "        print(f\"The RMSE for {model_name} ({rmse:.2f}) is equal to the standard deviation of the target variable ({std_y_test:.2f}), suggesting that the model is performing similarly to predicting the mean value.\")\n",
    "    else:\n",
    "        print(f\"The RMSE for {model_name} ({rmse:.2f}) is higher than the standard deviation of the target variable ({std_y_test:.2f}), suggesting that the model is not performing well.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 Select the Best Performing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest RMSE\n",
    "best_model = min(rmse_values, key=rmse_values.get)\n",
    "print(f\"The best-performing model is: {best_model} with an RMSE of {rmse_values[best_model]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Interpretation of Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Performance Evaluation\n",
    "The model performance was assessed using Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) both before and after hyperparameter tuning. The post-tuning results are as follows:\n",
    "\n",
    "- **Random Forest:**  \n",
    "  - MSE = **67.81**  \n",
    "  - RMSE = **8.23**  \n",
    "\n",
    "- **Support Vector Regression (SVR):**  \n",
    "  - MSE = **85.17**  \n",
    "  - RMSE = **9.23**  \n",
    "\n",
    "- **Gradient Boosting:**  \n",
    "  - MSE = **76.12**  \n",
    "  - RMSE = **8.72**  \n",
    "\n",
    "Among the three models, Random Forest performed the best, achieving the lowest RMSE (8.23), which suggests it had the most accurate predictions for crown closure. SVR had the highest error, indicating it struggled more with the dataset.\n",
    "\n",
    "##### Model Performance vs. Expectations\n",
    "The models performed reasonably well. An RMSE of 8-9 means that the predicted crown closure values deviate from the actual values by about 8-9 percentage points on average. Given the complexity of forest structure, this level of error seems acceptable.\n",
    "\n",
    "##### Deployment Feasibility\n",
    "Based on these results, Random Forest appears to be the most viable model for deployment. If crown closure predictions were being used for habitat modeling or forestry planning, further validation would be necessary.\n",
    "\n",
    "##### Improvements for Better Performance\n",
    "To improve model accuracy, the following steps could be taken:\n",
    "\n",
    "- **Hyperparameter Fine-tuning:** Further optimization, especially for Gradient Boosting, might yield better results.  \n",
    "- **Alternative Models:** Trying other models such as neural networks or ensemble stacking could enhance performance.  \n",
    "\n",
    "##### Effect of Hyperparameter Tuning\n",
    "MSE Results before and after tuning:\n",
    "\n",
    "- **Random Forest:**  \n",
    "  - Before tuning: MSE = **-68.29**  \n",
    "  - After tuning: MSE = **67.81**  \n",
    "\n",
    "- **Support Vector Regression (SVR):**  \n",
    "  - Before tuning: MSE = **-111.12**  \n",
    "  - After tuning: MSE = **85.17**  \n",
    "\n",
    "- **Gradient Boosting:**  \n",
    "  - Before tuning: MSE = **-90.09**  \n",
    "  - After tuning: MSE = **76.12**  \n",
    "\n",
    "**Summary:**\n",
    "  - Hyperparameter tuning has successfully improved model performance, leading to better MSE values for all three models.\n",
    "  - Random Forest was the best performer after tuning, followed by Gradient Boosting, and then SVR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
